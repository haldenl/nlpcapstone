{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating human 'attention' from source / summary input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p_gens', 'article_lst', 'abstract_str', 'decoded_lst', 'attn_dists'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "f = open('/Users/haldenl/nlpcapstone/data/attn_vis_data.json', 'r')\n",
    "data = json.load(f)\n",
    "\n",
    "print(data[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax function, from @nolanbconaway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta = 1.0, axis = None):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each element along an axis of X.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ND-Array. Probably should be floats. \n",
    "    theta (optional): float parameter, used as a multiplier\n",
    "        prior to exponentiation. Default = 1.0\n",
    "    axis (optional): axis to compute values along. Default is the \n",
    "        first non-singleton axis.\n",
    "\n",
    "    Returns an array the same size as X. The result will sum to 1\n",
    "    along the specified axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter, \n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculates the similarity betweens sentences in article and summary (softmaxed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceSimilarities(article_sentences, human_sentences):\n",
    "    sentence_similarities = np.zeros([len(human_sentences), len(article_sentences)])\n",
    "    \n",
    "    \n",
    "    for human_index, human_sent in enumerate(human_sentences):\n",
    "        for article_index, article_sent in enumerate(article_sentences):\n",
    "            a_sent = article_sent\n",
    "            h_sent = human_sent\n",
    "            \n",
    "            similarity = h_sent.similarity(a_sent)\n",
    "            sentence_similarities[human_index][article_index] = similarity\n",
    "        \n",
    "    sentence_similarities = softmax(sentence_similarities, theta=20, axis=1)\n",
    "    return sentence_similarities\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculates 'attention' weights between article and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeights(article, human):\n",
    "    weights = np.zeros([len(human), len(article)])\n",
    "    \n",
    "    article_sentences = list(article.sents)\n",
    "    human_sentences = list(human.sents)\n",
    "        \n",
    "    sentence_similarities = getSentenceSimilarities(article_sentences, human_sentences)\n",
    "    \n",
    "    count = 0\n",
    "    human_rows = {}\n",
    "    for human_index, human_sent in enumerate(human_sentences):\n",
    "        human_rows[human_index] = {}\n",
    "        for human_tok_index, human_token in enumerate(human_sent):\n",
    "            human_rows[human_index][human_tok_index] = count\n",
    "            count += 1\n",
    "            \n",
    "    count = 0\n",
    "    article_columns = {}\n",
    "    for article_index, article_sent in enumerate(article_sentences):\n",
    "        article_columns[article_index] = {}\n",
    "        for article_tok_index, article_token in enumerate(article_sent):\n",
    "            article_columns[article_index][article_tok_index] = count\n",
    "            count += 1\n",
    "    \n",
    "    for human_index, human_sent in enumerate(human_sentences):\n",
    "        human_sent = nlp(human_sent.text)\n",
    "        \n",
    "        for article_index, article_sent in enumerate(article_sentences):\n",
    "            article_sent = nlp(article_sent.text)\n",
    "        \n",
    "            sentence_sim = sentence_similarities[human_index][article_index]\n",
    "            \n",
    "            for human_tok_index, human_token in enumerate(human_sent):\n",
    "                count += 1\n",
    "                for article_tok_index, article_token in enumerate(article_sent):\n",
    "                    similarity = 0\n",
    "                    if (len(human_token.text) == 1 or len(article_token.text) == 1):\n",
    "                        similarity = 0.1 * random.random()\n",
    "                    else:\n",
    "                        similarity = human_token.similarity(article_token)\n",
    "                        \n",
    "                        \n",
    "                    weight = sentence_sim * similarity\n",
    "    \n",
    "                    row = human_rows[human_index][human_tok_index]\n",
    "                    column = article_columns[article_index][article_tok_index]\n",
    "                    weights[row][column] = weight                  \n",
    "                  \n",
    "    weights = softmax(weights, theta=100, axis=1)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_str=\"A super slimmer who swelled to 26 stone after eating a loaf of bread a day is now toasting her diet - and literally becoming half the woman she used to be. Michelle Quinn, 42, ballooned after munching her way through toast, sarnies and slices of bread all day. Her diet used to be made up of white toast with margarine for breakfast, sandwiches and crisps for lunch and fish and chips and takeaways for dinner. Michelle Quinn has lost half of her body weight after ditching her bread-based diet which saw her eat a loaf a day, she has also dropped from a clothes size 30 (left) to a size 12 (right) Before losing weight 43-year-old Michelle was a size 30, here she proudly holds up a pair of her old trousersÂ But she gave up the bread and started a diet of breakfast of cereal or fruit and yoghurt, home-made soup for lunch and healthy versions of her favourite meals. Michelle, of South Shields, Tyneside, says she feels like a new woman after losing 12.5st and dropping from dress size 30 to size 12. She has been named Slimming World's Greatest Loser in the West Harton area of South Shields. She said: 'I feel like a new woman since losing weight. In fact, I look so different that people who I havent seen for a while often can't believe I'm the same person. 'For me though it's the change on the inside that's been the biggest - I'm happier, healthier and much more confident now. Michelle, pictured with her uncle Derek, joined a slimming group in 2013 in a bid to shift the weight, she says she had struggled with high blood pressure, back pain and that she got breathless easily Michelle was not fat as a child but piled on the pounds thanks to her diet of fish and chips and sandwhiches Now a size 12, Michelle no longer feels the need to eat a whole loaf of bread every day She continued: 'I still enjoy all my favourite meals like burgers and chips and roast dinners but I've learned how to make small changes like using lean meat or cooking with low calorie spray instead of oil or butter. 'It fits in really well with the rest of my family and we can all eat the same meals.' Michelle weighed 25st 3lbs when she joined the group in April 2013 and has since dropped to 12st 10lbs. She said: 'Before I lost the weight I hid behind a big bubbly personality. I'd pretend it didn't bother me that I was bigger than most other people, but that was far from the truth. 'I hated shopping for clothes and found just climbing up stairs and doing simple everyday tasks would leave me tired and out of breath.' Michelle's weight was also putting a huge strain on her health and she suffered with high blood pressure, chronic back pain and got breathless easily.\"\n",
    "human_str=\"Michelle Quinn ballooned thanks to her addiction to bread and chips The 42-year-old would eat a whole loaf of bread a day In 2013 she weighed more than 25st and was a size 30 dress size After joining a slimming group her weight has dropped to 12 stone 10lbs\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = nlp(article_str.lower())  \n",
    "human = nlp(human_str.lower())\n",
    "\n",
    "weights = getWeights(article, human)\n",
    "\n",
    "attentionRecords = []\n",
    "inputRecords = []\n",
    "outputRecords = []\n",
    "\n",
    "for human_index, human_token in enumerate(human):\n",
    "    for article_index, article_token in enumerate(article):\n",
    "        \n",
    "        attn_weight = weights[human_index][article_index]\n",
    "                \n",
    "        attentionRecords.append({\n",
    "            'inputIndex': article_index,\n",
    "            'outputIndex': human_index,\n",
    "            'weight': attn_weight\n",
    "        })\n",
    "        \n",
    "for human_index, human_token in enumerate(human):\n",
    "    outputRecords.append({\n",
    "        'index': human_index,\n",
    "        'token': human_token.text\n",
    "    })\n",
    "    \n",
    "for article_index, article_token in enumerate(article):\n",
    "    inputRecords.append({\n",
    "        'index': article_index,\n",
    "        'token': article_token.text\n",
    "    })\n",
    "            \n",
    "output = {\n",
    "    'attentionRecords': attentionRecords,\n",
    "    'inputTokens': inputRecords,\n",
    "    'outputTokens': outputRecords\n",
    "}\n",
    "\n",
    "with open('/Users/haldenl/nlpcapstone/data/hierarchical_similarity_data_{0}.json'.format('michelle_3'), 'w') as out:\n",
    "    json.dump(output, out, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
