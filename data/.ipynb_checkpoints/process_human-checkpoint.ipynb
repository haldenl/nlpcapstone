{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating human 'attention' from source / summary input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p_gens', 'article_lst', 'abstract_str', 'decoded_lst', 'attn_dists'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "f = open('/Users/haldenl/nlpcapstone/data/attn_vis_data.json', 'r')\n",
    "data = json.load(f)\n",
    "\n",
    "print(data[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax function, from @nolanbconaway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta = 1.0, axis = None):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each element along an axis of X.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ND-Array. Probably should be floats. \n",
    "    theta (optional): float parameter, used as a multiplier\n",
    "        prior to exponentiation. Default = 1.0\n",
    "    axis (optional): axis to compute values along. Default is the \n",
    "        first non-singleton axis.\n",
    "\n",
    "    Returns an array the same size as X. The result will sum to 1\n",
    "    along the specified axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter, \n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculates the similarity betweens sentences in article and summary (softmaxed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceSimilarities(article_sentences, human_sentences):\n",
    "    sentence_similarities = np.zeros([len(human_sentences), len(article_sentences)])\n",
    "    \n",
    "    \n",
    "    for human_index, human_sent in enumerate(human_sentences):\n",
    "        for article_index, article_sent in enumerate(article_sentences):\n",
    "            a_sent = article_sent\n",
    "            h_sent = human_sent\n",
    "            \n",
    "            similarity = h_sent.similarity(a_sent)\n",
    "            sentence_similarities[human_index][article_index] = similarity\n",
    "        \n",
    "    sentence_similarities = softmax(sentence_similarities, theta=100, axis=1)\n",
    "    return sentence_similarities\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculates 'attention' weights between article and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeights(article, human):\n",
    "    weights = np.zeros([len(human), len(article)])\n",
    "    \n",
    "    article_sentences = list(article.sents)\n",
    "    human_sentences = list(human.sents)\n",
    "        \n",
    "    sentence_similarities = getSentenceSimilarities(article_sentences, human_sentences)\n",
    "    \n",
    "    count = 0\n",
    "    human_rows = {}\n",
    "    for human_index, human_sent in enumerate(human_sentences):\n",
    "        human_rows[human_index] = {}\n",
    "        for human_tok_index, human_token in enumerate(human_sent):\n",
    "            human_rows[human_index][human_tok_index] = count\n",
    "            count += 1\n",
    "            \n",
    "    count = 0\n",
    "    article_columns = {}\n",
    "    for article_index, article_sent in enumerate(article_sentences):\n",
    "        article_columns[article_index] = {}\n",
    "        for article_tok_index, article_token in enumerate(article_sent):\n",
    "            article_columns[article_index][article_tok_index] = count\n",
    "            count += 1\n",
    "    \n",
    "    for human_index, human_sent in enumerate(human_sentences):\n",
    "        human_sent = nlp(human_sent.text)\n",
    "        \n",
    "        for article_index, article_sent in enumerate(article_sentences):\n",
    "            article_sent = nlp(article_sent.text)\n",
    "        \n",
    "            sentence_sim = sentence_similarities[human_index][article_index]\n",
    "            \n",
    "            for human_tok_index, human_token in enumerate(human_sent):\n",
    "                count += 1\n",
    "                for article_tok_index, article_token in enumerate(article_sent):\n",
    "                    similarity = 0\n",
    "                    if (len(human_token.text) == 1 or len(article_token.text) == 1):\n",
    "                        similarity = 0.1 * random.random()\n",
    "                    else:\n",
    "                        similarity = human_token.similarity(article_token)\n",
    "                        \n",
    "                        \n",
    "                    weight = sentence_sim * similarity\n",
    "    \n",
    "                    row = human_rows[human_index][human_tok_index]\n",
    "                    column = article_columns[article_index][article_tok_index]\n",
    "                    weights[row][column] = weight                  \n",
    "                  \n",
    "    weights = softmax(weights, theta=100, axis=1)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_str=\"March 31, 2015 Iran's controversial nuclear program, U.S. oil production, and the wealth of American presidents are three topics covered this Tuesday on CNN Student News. We're delivering a random fact about the height of a historic conqueror, and we're examining what it takes for rescue dogs to locate avalanche victims. And we have a follow-up on a pair of eagles who are excellent parents. On this page you will find today's show Transcript and a place for you to request to be on the CNN Student News Roll Call. TRANSCRIPT Click here to access the transcript of today's CNN Student News program. Please note that there may be a delay between the time when the video is available and when the transcript is published. CNN Student News is created by a team of journalists who consider the Common Core State Standards, national standards in different subject areas, and state standards when producing the show. ROLL CALL For a chance to be mentioned on the next CNN Student News, comment on the bottom of this page with your school name, mascot, city and state. We will be selecting schools from the comments of the previous show. You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call! Thank you for using CNN Student News!\"\n",
    "human_str=\"This page includes the show Transcript Use the Transcript to help students with reading comprehension and vocabulary At the bottom of the page, comment for a chance to be mentioned on CNN Student News.  You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = nlp(article_str.lower())  \n",
    "human = nlp(human_str.lower())\n",
    "\n",
    "weights = getWeights(article, human)\n",
    "\n",
    "attentionRecords = []\n",
    "inputRecords = []\n",
    "outputRecords = []\n",
    "\n",
    "for human_index, human_token in enumerate(human):\n",
    "    for article_index, article_token in enumerate(article):\n",
    "        \n",
    "        attn_weight = weights[human_index][article_index]\n",
    "                \n",
    "        attentionRecords.append({\n",
    "            'inputIndex': article_index,\n",
    "            'outputIndex': human_index,\n",
    "            'weight': attn_weight\n",
    "        })\n",
    "        \n",
    "for human_index, human_token in enumerate(human):\n",
    "    outputRecords.append({\n",
    "        'index': human_index,\n",
    "        'token': human_token.text\n",
    "    })\n",
    "    \n",
    "for article_index, article_token in enumerate(article):\n",
    "    inputRecords.append({\n",
    "        'index': article_index,\n",
    "        'token': article_token.text\n",
    "    })\n",
    "            \n",
    "output = {\n",
    "    'attentionRecords': attentionRecords,\n",
    "    'inputTokens': inputRecords,\n",
    "    'outputTokens': outputRecords\n",
    "}\n",
    "\n",
    "with open('/Users/haldenl/nlpcapstone/data/hierarchical_similarity_data_{0}.json'.format('women_1'), 'w') as out:\n",
    "    json.dump(output, out, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
